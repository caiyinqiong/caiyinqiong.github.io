```
layout:     post
title:      个人杂谈之EMNLP2019
date:       2020-02-13
author:     cyq
catalog: true
tags:
    - 个人杂谈
```





# MRC

- Ranking and Sampling in Open-Domain Question Answering

  > 内容简介：本文针对开放域QA问题。 以往的方法一般使用question-passage的相关度来检索evidence passage，但是忽略了也可以利用passage-passage之间的相关度，而且如果在训练时只用高置信度的段落进行训练也会影响模型的泛化能力。本文提出一个排序模型，利用question-passage和passage-passage之间的相关度来计算每个段落的置信度，然后基于这个置信度设计了一个修正的weighted sampling strategy来选择训练段落，缓解噪声段落和干扰段落的影响，提高模型的泛化性。





# 检索





# 对话

- Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots

  > 内容简介：通常检索式的对话系统都是把所有对话历史与候选答案进行匹配，但本文论点认为很多历史对话和当前对话不太相关，反而会引入噪声信息影响对当前正确答案的判断。因此提出，先拿最后一个utterance（或者最后几个utterance）与所有历史对话进行相关性判断，对一些不相关的历史对话进行过滤，再拿过滤后的历史对话与候选答案进行匹配。
  >
  > 一点想法：这样的论点具有合理性，方法简洁有效。

- Multi-Granularity Representations of Dialog

  > 内容简介：在集成学习的框架下，通过与ground-truth response的距离远近，采样多组负例候选答案，构成多个训练集，训练得到多个模型。不同的模型可以显式学习到不同粒度的表达，从而提高response retrieval的性能。

- PullNet：Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text

  > 内容简介：本文主要以KB和corpus作为外部知识，进行多跳推理的QA。采用迭代式的构图方式，每步用GCN选择需要被扩展的实体节点，再根据该节点和问题来检索相关的文本或者事实三元组，进而更新图。当图构建完成后，再用一个GCN选择答案。





# NMT

- Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation

  > 内容简介：采用分层的encoder模型（sentence-encoder + document-encoder），显式地在词级别的上下文表示中加入global context。。同时考虑到域内的平行语料不多，所以采用分块的两阶段训练方式。



# 其他

- Enhancing Local Feature Extraction with Global Representation for Neural Text Classification

  > 内容简介：通常文本分类任务都是提取很多局部特征，然后一起送到分类器中。但本文论点认为不考虑全局信息的局部特征无法具有instance-specific性。因为提出，结合全局特征提取器和局部特征提取器两个组件，在提取局部特征时融合进全局特征。
  >
  > 一点想法：这样的论点具有合理性，方法简洁有效。

- Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations

  > 内容简介：本文提出了DiscoEval评估基准，一共包含7个任务，主要用于评测常用的编码器得到的句子表示是否很好地包含了篇章信息。。。此外，还提出了一些针对篇章信息的多任务训练目标，可以帮助学到的句子表示捕捉不同的篇章信息。

- Parameter-free Sentence Embedding via Orthogonal Basis

  > 内容简介：本文提出基于预训练的词向量，用非参数化的方法学习句子的向量表示。基于三个方面来衡量每个词对于整个句子表示的贡献度：1）每个词自身的novelty；2）每个词的novelty与其他正交基的对齐度；3）每个词与整个corpus的对齐度。。。。。但是提的计算公式很奇怪，，虽然从结果看有一定提升。









