---
layout:     post
title:      面试题
date:       2020-01-30
author:     cyq
catalog: true
tags:
    - 找工作
    - 面试题
---

# 卷积

#### 1. causal conv

当我们对输入数据的顺序很注重的时候，因果卷积（causal conv）便可以发挥作用。

Causal最初跟随WaveNets一起提出。WaveNets是一个生成模型，主要用来生成音乐。WaveNets是利用卷积来学习t时刻之前的输入数据（音频），来预测t+1时刻的输出。也就是说，该模型输出的最后的X的概率会是如公式-3 所示。在公式-3 和PPT中的动态图片中，我们可以看出，t时刻的输出仅仅依赖于1,2,…,t-1时刻的输入，不会依赖于t+1时刻以及之后时刻的输入。这与BiLSTM的思想截然不同。

<img src="../../notes/images/image-20200201160530063.png" alt="image-20200201160530063" style="zoom:50%;" />

当你的模型有这种特殊要求时，便可以采用casual。

在实现上，1D的casual 主要是通过padding来实现的。在2D的casual 主要是通过mask filter map来实现的。

![img](https://images2018.cnblogs.com/blog/1020673/201807/1020673-20180731173101855-1394567974.gif)

#### 2. dilated conv

Dilated conv在ICLR 2016上提出。其主要作用是在不增加参数和模型复杂度的条件下，可以指数倍的扩大视觉野的大小。

下图所示，是kernel_size=2，strides=1，padding=0，分别经过4层dilated={1,2,4,8}的卷积后，得到一个output值。

![image-20200201160924125](../../notes/images/image-20200201160924125.png)

假设kernel_size=m，dilated=d，则膨胀之后的卷积核大小为(m-1)*d+1。

#### 3. 感受野的计算

<img src="../../notes/images/image-20200201162423770.png" alt="image-20200201162423770" style="zoom:40%;" />



















